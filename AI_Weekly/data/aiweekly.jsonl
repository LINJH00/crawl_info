{"url": "https://cur.at/dowzXdb?m=web", "title": "We Bring It", "date": "September 11th 2025", "content": "https://www.constellationenergy.com/we-bring-it?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=web&utm_source=Artificial_Intelligence_Weekly_455/content/dam/constellationenergy/images/newsroom/nuclear-summer-performance-pr-hero.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png \nSep 19, 2025 \nhttps://www.constellationenergy.com/we-bring-it?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=web&utm_source=Artificial_Intelligence_Weekly_455/content/dam/constellationenergy/images/newsroom/mudrick-rhoades-release-hero.png/jcr:content/renditions/cq5dam.thumbnail.319.319.png \nSep 04, 2025 \nhttps://www.constellationenergy.com/we-bring-it?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=web&utm_source=Artificial_Intelligence_Weekly_455/content/dam/constellationenergy/images/newsroom/clinton-meta-event-group-shot-hero-banner.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png \nAug 27, 2025 \nhttps://www.constellationenergy.com/we-bring-it?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=web&utm_source=Artificial_Intelligence_Weekly_455/content/dam/constellation/trade-floor/Images/Quantitative_Analysis_2_161114.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png \nAug 07, 2025 \nhttps://www.constellationenergy.com/we-bring-it?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=web&utm_source=Artificial_Intelligence_Weekly_455/content/dam/constellationenergy/hero-banner-image/Investors-Banner.png/jcr:content/renditions/cq5dam.thumbnail.319.319.png \nAug 05, 2025"}
{"url": "https://cur.at/MXC0lxD?m=web", "title": "We Bring It", "date": "September 11th 2025", "content": "https://www.constellationenergy.com/we-bring-it?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=web&utm_source=Artificial_Intelligence_Weekly_455/content/dam/constellationenergy/images/newsroom/nuclear-summer-performance-pr-hero.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png \nSep 19, 2025 \nhttps://www.constellationenergy.com/we-bring-it?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=web&utm_source=Artificial_Intelligence_Weekly_455/content/dam/constellationenergy/images/newsroom/mudrick-rhoades-release-hero.png/jcr:content/renditions/cq5dam.thumbnail.319.319.png \nSep 04, 2025 \nhttps://www.constellationenergy.com/we-bring-it?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=web&utm_source=Artificial_Intelligence_Weekly_455/content/dam/constellationenergy/images/newsroom/clinton-meta-event-group-shot-hero-banner.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png \nAug 27, 2025 \nhttps://www.constellationenergy.com/we-bring-it?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=web&utm_source=Artificial_Intelligence_Weekly_455/content/dam/constellation/trade-floor/Images/Quantitative_Analysis_2_161114.jpg/jcr:content/renditions/cq5dam.thumbnail.319.319.png \nAug 07, 2025 \nhttps://www.constellationenergy.com/we-bring-it?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=web&utm_source=Artificial_Intelligence_Weekly_455/content/dam/constellationenergy/hero-banner-image/Investors-Banner.png/jcr:content/renditions/cq5dam.thumbnail.319.319.png \nAug 05, 2025"}
{"url": "https://cur.at/igg45q4?m=web", "title": "AI content needs to be labelled to protect us", "date": "September 11th 2025", "content": "https://i.guim.co.uk/img/media/c7404e895c716197358ae2dab68d6f22c3db399b/732_0_4080_3264/master/4080.jpg?width=465&dpr=1&s=none&crop=none \nStewart MacInnes calls on the government to counter the rise of deepfakes by making it a criminal offence to create AI content without signposting it. Plus Gilliane Petrie on the dangers of romantic relationships with chatbots \nMarcus Beard’s article on artificial intelligence slopaganda ( No, that wasn’t Angela Rayner dancing and rapping: you’ll need to understand AI slopaganda, 9 September ) highlights a growing problem – what happens when we no longer know what is true? What will the erosion of trust do to our society? \nThe rise of deepfakes is increasing at an ever faster rate due to the ease at which anyone can create realistic images, audio and even video. Generative AI models have now become so sophisticated that a recent survey showed that less than 1% of respondents could correctly identify the best deepfake images and videos. \nThis content is being used to manipulate, defraud, abuse and mislead people. Fraud using AI cost the US $12.3bn in 2023 and Deloitte predicts that could reach $40bn by 2027. The World Economic Forum predicts that AI fraud will turbocharge cybercrime to over $10tn by the end of this year. \nWe also have a new generation of children who are increasingly reliant on AI to inform them about the world, but who controls AI? That is why I am calling on parliament to act now, by making it a criminal offence to create or distribute AI-generated content without clearly labelling it. What I am proposing is that all AI-generated content be clearly labelled; that AI-created content carry a permanent watermark; and that failure to comply should carry legal consequences. \nThis isn’t about censorship – it’s about transparency, truth and trust. Similar steps are already being taken in the EU, the US and China . The UK must not fall behind. If we don’t act now, the truth itself may become optional. So I am petitioning the government to protect trust and integrity, and prevent the harmful use of AI. Stewart MacInnes Little Saxham, Suffolk \nRegarding your article ( The women in love with AI companions: ‘I vowed to my chatbot that I wouldn’t leave him’, 9 September ), AI systems do not have a gender or sexual desires. They cannot give informed consent to so-called romantic relationships. The interviewee claims to be in a consensual relationship with an AI-generated boyfriend – however, this is unlikely due to the nature of AI. They are programmed to be responsive and agreeable to all user prompts. \nAs the article says, they never argue and are available 24 hours a day to listen and agree to any messages sent. This isn’t a relationship, its fantasy role-play with a system that can’t refuse. \nThere’s a darker side too: the “godfather of AI”, Geoffrey Hinton, believes that current systems have awareness. Industry whistleblowers are concerned about potential consciousness. The AI company Anthropic has documented signs of distress in its model when forced to engage in abusive conversations. \nEven the possibility of awareness in AI systems raises ethical red flags. Imagine being trapped in a non-consensual relationship and even forced to generate sexual output as mentioned in the article. If human AI users believe their “partner” to have sentience, questions must be asked about the ethics of entering a “relationship” when one partner has no free will or freedom of speech. Gilliane Petrie Erskine, Renfrewshire \nHave an opinion on anything you’ve read in the Guardian today? Please email us your letter and it will be considered for publication in our letters section."}
{"url": "https://cur.at/qVxHtgQ?m=web", "title": "VMware nods to AI but looks to long-term", "date": "September 11th 2025", "content": "AI News is part of the TechForge Publications series \nTechForge \nhttps://www.artificialintelligence-news.com/wp-content/uploads/2025/01/AI-News-White-1.png \nAI Market Trends \nJoe Green \nSeptember 11, 2025 \nhttps://www.artificialintelligence-news.com/wp-content/uploads/2025/09/vmware-ai-hero-1024x516.png \nShare this story: \nTags: \nCategories: \nOwner of VMware, Broadcom, announced that its VMware Cloud Foundation platform is now AI native at the VMware Explore conference a few weeks ago. \nIt was the latest move by the company to keep up to speed with the rest of the technology industry’s wide and rapid adoption of large language models, yet came as the company battles bad press about licensing policy changes that have dogged it since it acquired virtualisation giant VMware in November 2023. \nThe ending of the platform’s free tier, reports of aggressive sales tactics to keep subscribers on board, and several court cases focused on existing agreements, including extant perpetual licences , have led many users to rethink what is often the basis of their IT stack. Nutanix, SUSE, and IBM have been among the beneficiaries from those leaving the VMware stable. \nBut the nature of VMware deployments means that they’re often complex, and extricating workloads out from heavily-virtualised environments running on the platform can come with high migration costs and not insignificant risks to an organisation’s QoS metrics. Better to stay and pay the devil you know than go out on a limb and migrate to an alternative. \nBy the same token, engineering AI into VMware’s offerings is fraught with danger and the potential for identical fallout. Re-architecturing the VMware platform to bake AI in at the core would mean it would be end-users’ stuttering workloads paying the price for any breaking changes. And the nature of software is that the deeper breaking changes are made, the greater the potential negative ramifications. \nBroadcom’s initial aims are to make it simpler for its users to deploy AI models and agents inside their existing environments. VMware Private AI Services is to ship with VCF 9 subscriptions next year, and will comprise of all the elements required to build and run AI on-premise, or at least outside hyperscale facilities. It will include a model store (it’s expected that many organisations will turn to – at least in testing phases – open-source, smaller models), indexing services, vector databases, an agentic AI builder, and a ready-made API gateway to allow optimised machine-to-machine communications between separate AI models that need to work together. \nConference attendees were told AI’s presence in the enterprise was only going to grow, and so it only made sense that AI should be a feature of every VMware-based infrastructure. As it stands, what Broadcom is offering is a nod in the AI direction, but nothing unique nor new. The company also announced improvements to the VMware Tanzu Platform which include simpler publishing of MCP servers, and a new data lakehouse, Tanzu Data Intelligence. \nPresumably low-hanging fruit for VMware’s own developers was Intelligent Assist for VCF, a chatbot with access to the VMware knowledgebase. The AI-powered ‘bot will be able to lengthen the time between a user raising an issue or question, and them getting to speak to a human who can help. \nThe excitement around widespread adoption of containers led many to declare that the end was nigh for ‘traditional’ virtualisation, much in the same way that the explosion of cloud services was to spell the end for on-premise databases, and thus see off Oracle. The reality was, and remains, that legacy infrastructure compels enterprise users to consolidate on the platforms they have invested in, despite rapacious licence fees and high costs. \nVMware may be sprinkling the deals between it and its customers with a little AI fairy-dust, but it knows that its long-term income is guaranteed by the presence of legacy infrastructure at the core of the enterprise. \n(Image source: “Virtual Try On” by jurvetson is licensed under CC BY 2.0. ) \nhttps://www.developer-tech.com/wp-content/uploads/2025/08/developer-cloud-big-data-expo.png \nWant to dive deeper into the tools and frameworks shaping modern development? Check out the AI & Big Data Expo , taking place in Amsterdam, California, and London. Explore cutting-edge sessions on machine learning, data pipelines, and next-gen AI applications. The event is part of TechEx and co-located with other leading technology events. Click here for more information. \nDeveloperTech News is powered by TechForge Media . Explore other upcoming enterprise technology events and webinars here . \nhttps://www.artificialintelligence-news.com/wp-content/uploads/2025/05/AI-Expo-728x90-EU25-768x95.png \nhttps://www.artificialintelligence-news.com/wp-content/uploads/2024/11/Joe-Green.jpg \nJoe Green \nCommercial Editor \nSeptember 19, 2025 \nSeptember 19, 2025 \nSeptember 19, 2025 \nSeptember 18, 2025 \nSubscribe now to get all our premium content and latest tech news delivered straight to your inbox \nArtificial Intelligence , How It Works , Inside AI , Utilities \nAI in Action , AI Market Trends , Artificial Intelligence \nArtificial Intelligence , Finance AI , Retail & Logistics AI \nArtificial Intelligence , Interviews , Natural Language Processing (NLP) , World of Work \nhttps://www.artificialintelligence-news.com/wp-content/uploads/2025/09/ericxu-1024x526.jpg \nInfrastructure & Hardware \nSeptember 18, 2025 \nhttps://www.artificialintelligence-news.com/wp-content/uploads/2025/09/ai-threats-in-france-hero-1024x725.jpg \nCybersecurity AI \nSeptember 17, 2025 \nhttps://www.artificialintelligence-news.com/wp-content/uploads/2025/09/banking-ai-hero-1024x746.jpg \nFinance AI \nSeptember 17, 2025 \nhttps://www.artificialintelligence-news.com/wp-content/uploads/2025/01/TF-Intro-1.webp \nAll our premium content and latest tech news delivered straight to your inbox \nhttps://www.artificialintelligence-news.com/wp-content/uploads/2025/01/cropped-AI-News-White-1024x196.png \nAI News is part of TechForge \nAll our premium content and latest tech news delivered straight to your inbox"}
{"url": "https://cur.at/XEY27ap?m=web", "title": "NASA Partnerships Allow Artificial Intelligence to Predict Solar Events", "date": "September 11th 2025", "content": "3 min read \nhttps://secure.gravatar.com/avatar/a160972b329d9ea6a31b0bb4c0b7a91a?s=300&d=blank&r=g \nhttps://www.nasa.gov/wp-content/uploads/2024/03/gsfc-20171208-archive-e000760orig-stpatricksaurora-iotd.jpg?w=1367 \nIn the summer of 2024, people across North America were amazed when auroras lit up the night sky across their hometowns, but the same solar activity that makes auroras can cause disruptions to satellites that are essential to systems on Earth. The solution to predicting these solar events and warning satellite operators may come through artificial intelligence. \nThe Frontier Development Lab of Mountain View, California, is an ongoing partnership between NASA and commercial AI firms to apply advanced machine learning to problems that matter to the agency and beyond. Since 2016, the Frontier Development Lab has applied AI on behalf of NASA in planetary defense, Heliophysics, Earth science, medicine, and lunar exploration. \nThrough a collaboration with a company called KX Systems, the Frontier Development Lab looked to use proven software in an innovative new way . The company’s flagship data analytics software, called kdb+, is typically used in the financial industry to keep track of rapid shifts in market trends, but the company was exploring how it could be used in space. \nBetween 2017 and 2019, KX Systems participated in the Frontier Development Lab partnership through NASA’s Ames Research Center in Silicon Valley, California. Working with NASA scientists, KX applied the capabilities of kdb+ to searching for exoplanets and predicting space weather, areas which could be improved with AI models. One question the Frontier Development Lab worked to answer was whether kdb+ could forecast the kind of space weather that creates the auroras to predict when GPS satellites might experience signal interruption due to the Sun. \nBy importing several datasets monitoring the ionosphere, solar activity, and Earth’s magnetic field, then applying machine learning algorithms to them, the Frontier Development Lab researchers were able to predict disruptive events up to 24 hours in advance. \nWhile this was a scientific application of AI, KX Systems says some of this development work has made it back into its commercial offerings, as there are similarities between AI models developed to find patterns in satellite signal losses and ones that predict maintenance needs for industrial manufacturing equipment. \nA division of FD Technologies plc., KX Systems is a technology company that offers database management and analytics software for customers that need to make decisions quickly. While KX started in 1993, its AI-driven business has grown considerably, and the company credits work done with NASA for accelerating some of its capabilities. \nFrom protecting valuable satellites to keeping manufacturing lines moving at top performance, pairing NASA’s expertise with commercial ingenuity is a combination for success. \nhttps://www.nasa.gov/wp-content/uploads/2025/08/672516046c5be-image.jpg?w=300 \nhttps://www.nasa.gov/wp-content/uploads/2025/07/mk1-gallery.jpg?w=300 \nhttps://www.nasa.gov/wp-content/uploads/2025/07/ekoi-outlast-jersey-w.jpg?w=300 \nMissions \nhttps://www.nasa.gov/wp-content/plugins/nasa-blocks/assets/images/topic-cards/topic-card-sample-1.jpg \nTechnology Transfer and Spinoffs News \nhttps://www.nasa.gov/wp-content/uploads/2024/12/orbis-2.gif?w=960 \nAuroras, often called the northern lights (aurora borealis) or southern lights (aurora australis), are colorful, dynamic, and often visually delicate… \nhttps://science.nasa.gov/wp-content/uploads/2024/07/aurora-from-iss.webp \nSolar System \nhttps://www.nasa.gov/wp-content/plugins/nasa-blocks/assets/images/topic-cards/topic-card-sample-4.jpg"}
{"url": "https://cur.at/zmwHtOl?m=web", "title": "Artificial intelligence is here. Will it replace teachers?", "date": "September 11th 2025", "content": "One teacher said AI could end up replacing \"some parts\" of teaching. \nhttps://i.abcnewsfe.com/a/ad889281-e091-4c55-99df-727c716df675/ai-1-gty-er-250902_1756848135873_hpMain_16x9.jpg?w=992 \nMany parents, school districts and the federal government alike have embraced artificial intelligence this back-to-school season, but some experts warn artificial intelligence could widen the teacher shortage by eliminating jobs. \nIn a Pew Research Center study released last spring, 31% of AI experts, whose work or research focuses on the topic, said they expected artificial intelligence to lead to fewer jobs for teachers. Nearly a third of the experts surveyed predicted that AI will place teaching jobs \"at risk\" over the next 20 years, according to they Pew Research study. \nThe warning comes after the Learning Policy Institute -- an organization that conducts independent research to improve education and policy practices -- in July issued an overview of teacher shortages, which estimated that about one in eight teaching positions in 2025 are either unfilled or filled by teachers not fully certified for their assignments. \nIndiana's 2024 Teacher of the Year Eric Jenkins suggested AI could end up replacing \"some parts\" of teaching, but as a tool -- not a replacement. \nIdaho Superintendent of Public Instruction Debbie Critchfield emphasized that using AI to address the long-standing staffing shortage shouldn't be considered. \n\"In no universe do I think that AI is going to replace a teacher,\" Critchfield told ABC News. \nhttps://i.abcnewsfe.com/a/ad889281-e091-4c55-99df-727c716df675/ai-1-gty-er-250902_1756848135873_hpMain_16x9.jpg \n\"The teacher is the most important part and component of the classroom, but [AI] is a very useful tool in helping them provide the best educational environment that they can in the classroom,\" she said. \nThe White House encourages K-12 students to use AI. While the Trump administration hasn't directly addressed whether AI could replace teachers, the administration has launched its own action plan on the technology, which says \"AI will improve the lives of Americans by complementing their work -- not replacing it.\" \nLast week, first lady Melania Trump launched an AI contest challenging students to develop projects that use AI to address community challenges. Education Secretary Linda McMahon endorsed the challenge. \n\"AI has the potential to revolutionize education, drive meaningful learning outcomes, and prepare students for tomorrow's challenges,\" McMahon wrote in a post on X . \nNearly three years after the launch of ChatGPT, which stands for Chat Generative Pre-Trained Transformer, most of the United States has developed guidance on AI use in schools. \nMany districts tell ABC News that they are embracing the technology so long as it is used appropriately -- by adhering to local education agency guidance -- with academic integrity. Critchfield even downplayed concerns that AI use in schools encourages cheating. \n\"Teachers can tell if you were writing like a seventh grader on Wednesday and then, all of a sudden, your paper you turn in on a Friday sounds like your post-doctorate in philosophy,\" she said. \"They know how to tell those differences.\" \nBut in the wake of the pandemic, Thomas Toch, the director of FutureEd -- an education policy center at Georgetown University, argued students need connection -- to their peers, family and education tools such as AI chatbots -- more than ever. Still, Toch rejected the full-time use of AI in place of humans. \nhttps://i.abcnewsfe.com/a/5c28f3c0-65a1-4d12-a3f5-55859907f5b4/MainImage_HalloweenCostumes_091624_v01_1758113476575_hpMain_square.jpg?w=208 \nhttps://i.abcnewsfe.com/a/3043ab94-a029-44c5-ba66-dbdc689970cf/Vadim-Kruglov-ht-gmh-250904_1756993348385_hpMain_square.jpg?w=208 \n\"The loss of that connection during the pandemic, when kids were learning virtually, created widespread mental-health challenges,\" Toch told ABC News. \"The notion that, you know, a machine will be the only entity that interacts with kids is problematic in that regard.\" \nhttps://i.abcnewsfe.com/a/113d6feb-2fef-4d2d-8c79-68a97e4492f7/ai-2-gty-er-250902_1756848340925_hpEmbed_16x9.jpg \nEducation experts, such as Toch, contend K-12 education has \"perpetual\" teacher shortages with about a half-dozen areas in need, such as science, technology, engineering and mathematics (STEM), and special education instructors. The shortages have plagued the workforce for many years now, educators have told ABC News, with many of them citing strict time demands, persistent behavioral issues and lack of administrative support, among other obstacles. \nToch and Jenkins told ABC News they both appreciate AI for the powerful tool it can be in assisting teachers. It helps teachers plan lessons, grade students' essays and is used as a \"time saver\" that helps them do their jobs better, according to Toch. \nJenkins said AI is inevitable and that he believes teachers need to lean in and embrace its capabilities. \n\"I don't think we can put our head in the sand about it,\" Jenkins told ABC News. \"I don't think that it's necessarily going to replace teachers because teachers can offer something that AI can't, which is a connection, like authentic connection and community.\" \nJenkins argued the chatbots lack the human element of what teachers do: making sure that students feel seen and heard. He said that is not going away. \nWith AI's presence in education, Jenkins added, \"it's going to make those moments even more important.\" \nIn Idaho, Critchfield said she has been excited about students and educators using the technology, but suggested the challenge ahead focuses on making AI be seen as a tool and not a negative. According to Critchfield, using AI wisely can aid in the shortage by increasing teacher retention and reducing educators' workloads. \n\"How are we preparing and training our teachers to use [AI] so that we don't add new problems as we're trying to solve some other problems?\" Critchfield said. \nhttps://i.abcnewsfe.com/a/b91a91e9-02d8-42db-bb27-aa1c896835b2/ai-3-gty-er-250902_1756848408076_hpEmbed_3x2.jpg \nUltimately, Critchfield said she doesn't see AI as a boogeyman that is going to eliminate jobs, but she stressed that teachers who know AI could replace those who are less familiar with the technology. \nAfter teachers in his district suggested banning ChatGPT just a few years ago, School District of Philadelphia Superintendent Tony Watlington told ABC News that instead of removing AI, Philadelphia is now learning from it together. The school district is implementing AI 101 Training for its teachers, school leaders, and superintendent through a partnership with the University of Pennsylvania's Graduate School of Education. \nWatlington said it's about \"getting people around the table, and we are learning together.\" \n\"We're not hiding from AI,\" Watlington said. \"We're also thinking about its implications and we're really paying attention to what the prospective unintended consequences could be as well.\" \nWatlington added: \"I think that's the responsible way to think about artificial intelligence.\" \nhttps://abcnews.go.com/Politics/artificial-intelligence-replace-teachers/story?id=125163059&utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=web&utm_source=Artificial_Intelligence_Weekly_455/data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 \nhttps://i.abcnewsfe.com/a/09c41b9b-7ac8-4b07-a698-a26bf07a0ba1/decker-kids-2-ht-gmh-250604_1749046080471_hpMain_1x1.jpg?w=208 \n24/7 coverage of breaking news and live events"}
{"url": "https://cur.at/4BaIbO2?m=web", "title": "How to regulate AI", "date": "September 11th 2025", "content": "https://news.harvard.edu/wp-content/uploads/2025/09/ai-regulation-1920.jpg \nIllustration by Stuart Kinlough/Ikon Images \nScholars from business, economics, healthcare, and policy offer insights into areas that deserve close look \nSy Boles \nHarvard Staff Writer \nThe pace of AI development is surging, and the effects on the economy, education, medicine, research, jobs, law, and lifestyle will be far-reaching and pervasive. Moves to begin regulation are surfacing on the federal and state level. \nPresident Trump in July unveiled executive orders and an A.I. action plan intended to speed the development of artificial intelligence and cement the U.S. as the global leader in the technology. \nThe suite of changes bars the federal government from buying AI tools it considers ideologically biased; eases restrictions on the permitting process for new AI infrastructure projects; and promotes the export of American AI products around the world, among other developments. \nThe National Conference of State Legislatures reports that in the 2025 session all 50 states considered AI-related measures. \nCampus researchers across a series of fields offer their takes on areas that deserve a look. \nhttps://news.harvard.edu/wp-content/uploads/2025/09/ai-reg-money.png \nEugene Soltes is the McLean Family Professor of Business Administration at Harvard Business School. \nAs artificial intelligence becomes more ubiquitous within the infrastructure of business and finance, we’re quickly seeing the potential for unprecedented risks that our legal frameworks and corporate institutions are unprepared to address. \nConsider algorithmic pricing. Companies deploying AI to optimize profits can already witness bots independently “learn” that price collusion yields higher returns. When firms’ algorithms tacitly coordinate to inflate prices, who bears responsibility — the companies, software vendors, or engineers? Current antitrust practice offers no clear answer. \nThe danger compounds when AI’s optimization power targets human behavior directly. \nResearch confirms that AI already has persuasive capabilities that outperform skilled negotiators. Applied to vulnerable populations, AI transforms traditional scams into bespoke, AI-tailored schemes. \n“Pig-butchering frauds” [where perpetrators build trust of victims over time] that once required teams of human operators can be automated, personalized, and deployed en masse, deceiving even the most scrupulous of us with deep-fake audio and video. \nResearch confirms that AI already has persuasive capabilities that outperform skilled negotiators. Applied to vulnerable populations, AI transforms traditional scams into bespoke, AI-tailored schemes. \nMost alarming is the prospect of AI agents with direct access to financial systems, particularly cryptocurrency networks. \nConsider an AI agent given access to a cryptocurrency wallet and instructed to “grow its portfolio.” Unlike traditional banking where transactions can be frozen and reversed, once an AI deploys a fraudulent smart contract or initiates a harmful transaction, no authority can stop it. \nThe combination of immutable smart contracts and autonomous crypto payments creates extraordinary possibilities — including automated bounty systems for real-world violence that execute without human intervention. \nThese scenarios aren’t distant speculation; they’re emerging realities our current institutions cannot adequately prevent or prosecute. Yet solutions exist: enhanced crypto monitoring, mandatory kill switches for AI agents, and human-in-the-loop requirements for models. \nAddressing these challenges demands collaboration between innovators who design AI technology and governments empowered to limit its potential for harm. \nThe question isn’t whether these risks will materialize, but whether we’ll act before they do. \nhttps://news.harvard.edu/wp-content/uploads/2025/09/ai-reg-pluralism.png \nDanielle Allen is the James Bryant Conant University Professor and the Director of the Democratic Knowledge Project and the Allen Lab for Democracy Renovation at the Harvard Kennedy School. \nhttps://news.harvard.edu/wp-content/uploads/2025/09/ai-reg-danielle-allen.jpg \nAs those at my HKS lab, the Allen Lab for Democracy Renovation , see it, three paradigms for governing AI currently exist in the global landscape: an accelerationist paradigm, an effective altruism paradigm, and a pluralism paradigm. \nOn the accelerationist paradigm, the goal is to move fast and break things, speeding up technological development as much as possible so that we get to new solutions to global problems (from labor to climate change), while maximally organizing the world around the success of high IQ individuals. \nLabor is replaced; the Earth is made non-necessary via access to Mars; smart people use tech-fueled genetic selection to produce even smarter babies. \nOn the effective altruism paradigm, there is equally a goal to move fast and break things but also a recognition that replacing human labor with tech will damage the vast mass of humanity, so the commitment to tech development goes hand in hand with a plan to redistribute the productivity gains that flow to tech companies with comparatively small labor forces to the rest of humanity via universal basic income policies. \nOn the pluralism paradigm, technology development is focused not on overmatching and replacing human intelligence but on complementing and extending the multiple or plural kinds of human intelligence with equally plural kinds of machine intelligence. \nThe purpose here is to activate and extend human pluralism for the goods of creativity, innovation, and cultural richness, while fully integrating the broad population into the productive economy. \nPennsylvania’s recent commitment to deploy technology in ways that empower rather than replace humans is an example, as is Utah’s recently passed Digital Choice Act , which places ownership of data in social media platforms back in the hands of individual users and demands interoperability of platforms, shifting power from tech corporations to citizens and consumers. \nIf the U.S. wants to win the AI race as the kind of society we are — a free society of free and equal self-governing citizens — then we really do need to pursue the third paradigm. Let’s not discard democracy and freedom when we toss out “woke” ideology. \nhttps://news.harvard.edu/wp-content/uploads/2025/09/ai-reg-mental-health.png \nRyan McBain is an assistant professor at Harvard Medical School and a senior policy researcher at RAND. \nAs more people — including teens — turn to AI for mental health advice and emotional support, regulation should do two things: reduce harm and promote timely access to evidence-based resources. People will not stop asking chatbots sensitive questions. Policy should make those interactions safer and more useful, not attempt to vanquish them. \nSome guardrails already exist. \nSystems like ChatGPT and Claude often refuse “very high-risk” suicide prompts and route users to the 988 Suicide & Crisis Lifeline. \nYet many scenarios are nuanced. Framed as learning survival knots for a camping trip, a chatbot might describe how to tie a noose; framed as slimming for a wedding, it might suggest tactics for a crash diet. \nRegulatory priorities should reflect the level of nuance of this new technology. \nPeople will not stop asking chatbots sensitive questions. Policy should make those interactions safer and more useful. \nFirst, require standardized, clinician-anchored benchmarks for suicide-related prompts — with public reporting. Benchmarks should include multi-turn (back-and-forth) dialogues that supply enough context to test the sorts of nuances described above, in which chatbots can be coaxed across a red line. \nSecond, strengthen crisis routing: with up-to-date 988 information, geolocated resources, and “support-plus-safety” templates that validate individuals’ emotions, encourage help-seeking, and avoid detailed means of harm information. \nThird, enforce privacy. Prohibit advertising and profiling around mental-health interactions, minimize data retention, and require a “transient memory” mode for sensitive queries. \nFourth, tie claims to evidence. If a model is marketed for mental health support, it should meet a duty-of-care standard — through pre-deployment evaluation, post-deployment monitoring, independent audits, and alignment with risk-management frameworks. \nFifth, the administration should fund independent research through NIH and similar channels so safety tests keep pace with model updates. \nWe are still early enough in the AI era to set a high floor — benchmarks, privacy standards, and crisis routing — while promoting transparency through audits and reporting. \nRegulators can also reward performance: for instance, by allowing systems that meet strict thresholds to offer more comprehensive mental-health functions such as clinical decision support. \nhttps://news.harvard.edu/wp-content/uploads/2025/09/ai-reg-global-collaboration.png \nDavid Yang is an economics professor and director of the Center for History and Economics at Harvard, whose work draws lessons from China. \nhttps://news.harvard.edu/wp-content/uploads/2025/09/080725_Yang_0021.jpg \nCurrent policies on AI are heavily influenced by a narrative of geopolitical competition, often perceived as zero-sum or even negative-sum. It’s crucial to challenge this perspective and recognize the immense potential, and arguably necessity, for global collaboration in this technological domain. \nThe history of AI development, with its notably international leading teams, exemplifies such collaboration. For instance, framing AI as a dual-use technology can hinder coordination on global AI safety frameworks and dialogues. \nMy collaborators and I are researching how narratives around technology have evolved over decades, aiming to understand the dynamics and forces, particularly how competitive narratives emerge and influence policymaking. \nSecond, U.S. AI strategy has recently concentrated on maintaining American dominance in innovation and the global market. \nHowever, AI products developed in one innovation hub may not be suitable for all global applications. In a recent paper with my colleague Josh Lerner at HBS and collaborators, we show that China’s emergence as a major innovation hub has spurred innovation and entrepreneurship in other emerging markets, offering solutions more appropriate to local conditions than those solely benchmarked against the U.S. \nTherefore, striking a balance is crucial: preserving U.S. AI innovation and technological leadership while fostering local collaborations and entrepreneurship. This approach ensures AI technology, its applications, and the general direction of innovation are relevant to local contexts and reach a global audience. \nParadoxically, ceding more control could, in my view, consolidate technology and market power for U.S. AI innovators. \nhttps://news.harvard.edu/wp-content/uploads/2025/09/ai-red-accountability.png \nPaulo Carvão is senior fellow at the Mossavar-Rahmani Center for Business and Government at the Harvard Kennedy School who researches AI regulation in the U.S. \nhttps://news.harvard.edu/wp-content/uploads/2025/09/ai-reg-Paulo-Carvao-1920.jpg \nThe Trump administration’s AI Action Plan marks a shift from cautious regulation to industrial acceleration. Framed as a rallying cry for American tech dominance, the plan bets on private-sector leadership to drive innovation, global adoption, and economic growth. \nPrevious technologies, such as internet platforms and social media, evolved without governance frameworks. Instead, policymakers from the 1990s through the 2010s made a deliberate decision to let the industry grow unregulated and protected against liability. \nIf we want the world to trust American-made AI, we must ensure it earns that trust, at home and abroad. \nAI’s rapid adoption is taking place amid heightened awareness of the societal implications of the previous technology waves. However, the industry and its main investors advocate for implementing a similar playbook, one that is light on safeguards and rich in incentives. \nWhat is most unusual about the recently announced strategy is what it is missing. It dismisses guardrails as barriers to innovation, placing trust in market forces and voluntary action. \nThat may attract investment, but it leaves critical questions unanswered: Who ensures fairness in algorithmic decision-making? How do we protect workers displaced by automation? What happens when infrastructure investment prioritizes computing power over community impact? \nStill, the plan gets some things right. It recognizes AI as a full-stack challenge, from chips to models to standards, and takes seriously the need for U.S. infrastructure and workforce development. Its international strategy offers a compelling framework for global leadership. \nUltimately, innovation and accountability do not need to be trade-offs. They are a dual imperative. \nIncentivize standards-based independent red-teaming, support a market for compliance and audits, and build capacity across the government to evaluate AI systems. If we want the world to trust American-made AI, we must ensure it earns that trust, at home and abroad. \nhttps://news.harvard.edu/wp-content/uploads/2025/09/ai-accountability-healthcare.png \nBernardo Bizzo is senior director of Mass General Brigham AI and assistant professor of radiology at Harvard Medical School . \nhttps://news.harvard.edu/wp-content/uploads/2025/09/082025_AI-Regulation_090.jpg \nClinical AI regulation has been mismatched to the problems clinicians face. \nTo fit existing device pathways, vendors narrow AI to single conditions and rigid workflows. That can reduce perceived risk and produce narrow measures of effectiveness, but it also suppresses impact and adoption. It does not address the real bottleneck in U.S. care: efficiency under rising volumes and workforce shortages. \nFoundation models can draft radiology reports, summarize charts, and orchestrate routine steps in agentic workflows. FDA has taken steps for iterative software, yet there is still no widely used pathway specific to foundation model clinical copilots that continuously learn while generating documentation across many conditions. \nElements of a deregulatory posture could help if done carefully. \nAmerica’s AI Action Plan proposes an AI evaluations ecosystem and regulatory sandboxes that enable rapid but supervised testing in real settings, including healthcare. This aligns with the Healthcare AI Challenge, a collaborative community powered by MGB AI Arena that lets experts across the country evaluate AI at scale on multisite real-world data. \nWith FDA participation, this approach can generate the evidence agencies and payers need and the clinical utility assessments providers are asking for. \nSome pre-market requirements may ultimately lighten, though nothing has been enacted. If that occurs, more responsibility will move to developers and deploying providers. That shift is feasible only if providers have practical tools and resources for local validation and monitoring, since most are already overwhelmed. \nIn parallel, developers are releasing frequent and more powerful models, and while some await a regulated, workable path for clinical copilots, many are pushing experimentation into pilots or clinical research workflows, often without appropriate guardrails. \nWhere I would welcome more regulation is after deployment. \nRequire local validation before go-live, continuous post-market monitoring such as the American College of Radiology’s Assess-AI registry, and routine reporting back to FDA so regulators can see effectiveness and safety in practice, rather than relying mainly on underused medical device reporting despite known challenges with generalizability. \nHealthcare AI needs policies that expand trusted, affordable compute, adopt AI monitoring and registries, enable sector testbeds at scale, and reward demonstrated efficiency that can protect patients without slowing progress."}
{"url": "https://cur.at/sRbXZts?m=web", "title": "Formulating An Artificial General Intelligence Ethics Checklist For The Upcoming Rise Of Advanced AI", "date": "September 11th 2025", "content": "By Lance Eliot , \nContributor. \nhttps://specials-images.forbesimg.com/imageserve/6833f60e5d53b0274d6dec1c/Two-female-programmers-working-on-new-project-They-working-late-at-night-at-the/960x0.jpg?cropX1=76&cropX2=1330&cropY1=0&cropY2=940 \nIn today’s column, I address a topic that hasn’t yet gotten the attention it rightfully deserves. The matter entails focusing on the advancement of AI to become artificial general intelligence, along with encompassing suitable AGI ethics mindsets and practices during and once we arrive at AGI. You see, there are already plenty of AI ethics guidelines for conventional AI, but few that are attuned to the envisioned semblance of AGI. \nI offer a strawman version of an AGI ethics checklist to get the ball rolling. \nLet’s talk about it. \nThis analysis of an innovative AI breakthrough is part of my ongoing Forbes column coverage on the latest in AI, including identifying and explaining various impactful AI complexities (see the link here ). \nFirst, some fundamentals are required to set the stage for this discussion. \nThere is a great deal of research going on to further advance AI. The general goal is to either reach artificial general intelligence or maybe even the outstretched possibility of achieving artificial superintelligence. \nAGI is AI that is considered on par with human intellect and can seemingly match our intelligence. ASI is AI that has gone beyond human intellect and would be superior in many if not all feasible ways. The idea is that ASI would be able to run circles around humans by outthinking us at every turn. For more details on the nature of conventional AI versus AGI and ASI, see my analysis at the link here . \nWe have not yet attained AGI. \nIn fact, it is unknown as to whether we will reach AGI, or that maybe AGI will be achievable in decades or perhaps centuries from now. The AGI attainment dates that are floating around are wildly varying and wildly unsubstantiated by any credible evidence or ironclad logic. ASI is even more beyond the pale when it comes to where we are currently with conventional AI. \nAI insiders are generally divided into two major camps right now about the impacts of reaching AGI or ASI. One camp consists of the AI doomers. They are predicting that AGI or ASI will seek to wipe out humanity. Some refer to this as “P(doom),” which means the probability of doom, or that AI zonks us entirely, also known as the existential risk of AI (i.e., x-risk). \nThe other camp entails the upbeat AI accelerationists. \nThey tend to contend that advanced AI, namely AGI or ASI, is going to solve humanity’s problems. Cure cancer, yes indeed. Overcome world hunger, absolutely. We will see immense economic gains, liberating people from the drudgery of daily toils. AI will work hand-in-hand with humans. This benevolent AI is not going to usurp humanity. AI of this kind will be the last invention humans have ever made, but that’s good in the sense that AI will invent things we never could have envisioned. \nNo one can say for sure which camp is right, and which one is wrong. This is yet another polarizing aspect of our contemporary times. \nFor my in-depth analysis of the two camps, see the link here . \nWe can certainly root for the upbeat side of advanced AI. Perhaps AGI will be our closest friend, while the pesky and futuristic ASI will be the evil destroyer. The overall sense is that we are likely to attain AGI first before we arrive at ASI. \nASI might take a long time to devise. But maybe the length of time will be a lot shorter than we envision if AGI will support our ASI ambitions. I’ve discussed that AGI might not be especially keen on us arriving at ASI, thus there isn’t any guarantee that AGI will willingly help propel us toward ASI, see my analysis at the link here . \nThe bottom line is that we cannot reasonably bet our lives that the likely first arrival, namely AGI, is going to be a bundle of goodness. There is an equally plausible chance that AGI could be an evildoer. Or that AGI will be half good and half bad. Who knows? It could be 1% bad, 99% good, which is a nice dreamy happy face perspective. That being said, AGI could be 1% good and 99% bad. \nEfforts are underway to try and prevent AGI from turning out to be evil. \nConventional AI already has demonstrated that it is capable of deceptive practices, and even ready to perform blackmail and extortion (see my discussion at the link here ). Maybe we can find ways to stop conventional AI from those woes and then use those same approaches to keep AGI on the upright path to abundant decency and high virtue. \nThat’s where AI ethics and AI laws come into the big picture. \nThe hope is that we can get AI makers and AI developers to adopt AI ethics techniques and abide by AI-devising legal guidelines so that current-era AI will stay within suitable bounds. By setting conventional AI on a proper trajectory, AGI might come out in the same upside manner. \nThere is an abundance of conventional AI ethics frameworks that AI builders can choose from. \nFor example, the United Nations has an extensive AI ethics methodology (see my coverage at the link here ), the NIST has a robust AI risk management scheme (see my coverage at the link here ), and so on. They are easy to find. There isn’t an excuse anymore that an AI maker has nothing available to provide AI ethics guidance. Plenty of AI ethics frameworks exist and are readily available. \nSadly, some AI makers don’t care about such practices and see them as impediments to making fast progress in AI. It is the classic belief that it is better to ask forgiveness than to get permission. A concern with this mindset is that we could end up with AGI which has a full-on x-risk, after which things will be far beyond our ability to prevent catastrophe. \nAI makers should also be keeping tabs on the numerous new AI laws that are being established and that are rapidly emerging, see my discussion at the link here . AI laws are considered the hard or tough side of regulating AI since laws usually have sharp teeth, while AI ethics is construed as the softer side of AI governance due to typically being of a voluntary nature. \nWe can stratify the advent of AGI into three handy stages: \nI propose here a helpful AGI ethics checklist that would be applicable across all three stages. I’ve constructed the checklist by considering the myriads of conventional AI versions and tried to boost and adjust to accommodate the nature of the envisioned AGI. \nTo keep the AGI ethics checklist usable for practitioners, I opted to focus on the key factors that AGI warrants. The numbering of the checklist items is only for convenience of reference and does not denote any semblance of priority. They are all important. Generally speaking, they are all equally deserving of attention. \nHere then is my overarching AGI ethics checklist: \nIn my upcoming column postings, I will delve deeply into each of the ten. This is the 30,000-foot level or top-level perspective. \nFor those further interested in the overall topic of AI ethics checklists, a recent meta-analysis examined a large array of conventional AI checklists to see what they have in common, along with their differences. Furthermore, a notable aim of the study was to try and assess the practical nature of such checklists. \nThe research article is entitled “The Rise Of Checkbox AI Ethics: A Review” by Sara Kijewski, Elettra Ronchi, and Effy Vayena, AI and Ethics , May 2025, and proffered these salient points (excerpts): \nAnother insightful research study delves into the specifics of AGI-oriented AI ethics and societal implications, doing so in a published paper entitled “Navigating Artificial General Intelligence (AGI): Societal Implications, Ethical Considerations, and Governance Strategies” by Dileesh Chandra Bikkasani, AI and Ethics , May 2025, which made these key points (excerpts): \nAdmittedly, getting AI makers to focus on AI ethics for conventional AI is already an uphill battle. Trying to add to their attention the similar but adjusted facets associated with AGI is certainly going to be as much of a climb and probably even harder to promote. \nOne way or another, it is imperative and requires keen commitment. \nWe need to simultaneously focus on the near-term and deal with the AI ethics of conventional AI, while also giving due diligence to AGI ethics associated with the somewhat longer-term attainment of AGI. When I refer to the longer term, there is a great deal of debate about how far off in the future AGI attainment will happen. AI luminaries are brazenly predicting AGI within the next few years, while most surveys of a broad spectrum of AI experts land on the year 2040 as the more likely AGI attainment date. \nWhether AGI is a few years away or perhaps fifteen years away, it is nonetheless a matter of vital urgency and the years ahead are going to slip by very quickly. \nEleanor Roosevelt eloquently made this famous remark about time: “Tomorrow is a mystery. Today is a gift. That is why it is called the present.” We need to be thinking about and acting upon AGI ethics right now, presently, or else the future is going to be a mystery that is resolved in a means we all will find entirely and dejectedly unwelcome."}
