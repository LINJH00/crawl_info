{"url": "https://huggingface.co/blog/inference-providers-scaleway", "title": "Scaleway on Hugging Face Inference Providers üî•", "content": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/6192895f3b8aa351a46fadfd/2VifD-AAKYk24AUmfSr_X.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/1655385361868-61b85ce86eb1f2c5e6233736.jpeg\nhttps://huggingface.co/avatars/51eb3ee3e4c3bcfd7b3255344e9b936c.svg\nhttps://huggingface.co/avatars/51eb3ee3e4c3bcfd7b3255344e9b936c.svg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/O2nLpiFPfUWBFbFdyTDus.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/67b6fa3a1a0bf9e8a711259a/Y0KZkw2fL0lUHg00NLw04.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/668d61aa17676d77918ba471/-0SH78MDDcdifu5kBFFJo.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ItPk2dGdlQ7w-WnJGahzy.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/6192895f3b8aa351a46fadfd/2VifD-AAKYk24AUmfSr_X.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/1608146735109-5fcfb7c407408029ba3577e2.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/1659336880158-6273f303f6d63a28483fde12.png\nhttps://huggingface.co/blog/assets/inference-providers/welcome-scaleway.jpg\nWe're thrilled to share that Scaleway is now a supported Inference Provider on the Hugging Face Hub!\nScaleway joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub‚Äôs model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers.\nThis launch makes it easier than ever to access popular open-weight models like gpt-oss , Qwen3 , DeepSeek R1 , and Gemma 3 ‚Äî right from Hugging Face. You can browse Scaleway's org on the Hub at https://huggingface.co/scaleway and try trending supported models at https://huggingface.co/models?inference_provider=scaleway&sort=trending .\nScaleway Generative APIs is a fully managed, serverless service that provides access to frontier AI models from leading research labs via simple API calls. The service offers competitive pay-per-token pricing starting at ‚Ç¨0.20 per million tokens.\nThe service runs on secure infrastructure located in European data centers (Paris, France), ensuring data sovereignty and low latency for European users. The platform supports advanced features including structured outputs, function calling, and multimodal capabilities for both text and image processing.\nBuilt for production use, Scaleway's inference infrastructure delivers sub-200ms response times for first tokens, making it ideal for interactive applications and agentic workflows. The service supports both text generation and embedding models. You can learn more about Scaleway's platform and infrastructure at https://www.scaleway.com/en/generative-apis/ .\nRead more about how to use Scaleway as an Inference Provider in its dedicated documentation page .\nSee the list of supported models here .\nhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-providers/user-settings-updated.png\nhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-providers/explainer.png\nhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-providers/model-widget-updated.png\nThe following example shows how to use Swiss AI's Apertus-70B using Scaleway as the inference provider. You can use a Hugging Face token for automatic routing through Hugging Face, or your own Scaleway API key if you have one.\nNote: this requires using a recent version of huggingface_hub (>= 0.34.6).\nHere is how billing works:\nFor direct requests, i.e. when you use the key from an inference provider, you are billed by the corresponding provider. For instance, if you use a Scaleway API key you're billed on your Scaleway account.\nFor routed requests, i.e. when you authenticate via the Hugging Face Hub, you'll only pay the standard provider API rates. There's no additional markup from us; we just pass through the provider costs directly. (In the future, we may establish revenue-sharing agreements with our provider partners.)\nImportant Note ‚ÄºÔ∏è PRO users get $2 worth of Inference credits every month. You can use them across providers. üî•\nSubscribe to the Hugging Face PRO plan to get access to Inference credits, ZeroGPU, Spaces Dev Mode, 20x higher limits, and more.\nWe also provide free inference with a small quota for our signed-in free users, but please upgrade to PRO if you can!\nWe would love to get your feedback! Share your thoughts and/or comments here: https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/49"}
{"url": "https://huggingface.co/blog/riskrubric", "title": "Democratizing AI Safety with RiskRubric.ai", "content": "https://huggingface.co/avatars/02a571bc791b78d3993d9a0484b70a29.svg\nhttps://huggingface.co/avatars/4e84ac947e91d9b42e5753440e3a90bc.svg\nhttps://huggingface.co/avatars/8f3fa0abf9ca323b27950da2c4161e44.svg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/64c13ee9e98a5e02c93459ee/o7huQQPXZ5vS8r5RVDvWP.png\nhttps://huggingface.co/avatars/7b68e250bc25e642d5b0e0699f4133f9.svg\nhttps://huggingface.co/avatars/725df414d7af56b3c3542a624bad7537.svg\nhttps://huggingface.co/avatars/95ba8fc9c73075d5b4112599cca63ce1.svg\nMore than 500,000 models can be found on the Hugging Face hub, but it‚Äôs not always clear to users how to choose the best model for them, notably on the security aspects. Developers might find a model that perfectly fits their use case, but have no systematic way to evaluate its security posture, privacy implications, or potential failure modes.\nAs models become more powerful and adoption accelerates, we need equally rapid progress in AI safety and security reporting. We're therefore excited to announce RiskRubric.ai , a novel initiative led by Cloud Security Alliance and Noma Security , with contributions by Haize Labs and Harmonic Security, for standardized and transparent risk assessment in the AI model ecosystem.\nRiskRubric.ai provides consistent, comparable risk scores across the entire model landscape , by evaluating models across six pillars: transparency, reliability, security, privacy, safety, and reputation.\nThe platform's approach aligns perfectly with open-source values: rigorous, transparent, and reproducible. Using Noma Security capabilities to automate the effort, each model undergoes:\nThese assessments produce 0-100 scores for each risk pillar, rolling up to clear A-F letter grades. Each evaluation also includes specific vulnerabilities found, recommended mitigations, and suggestions for improvements.\nRiskRubric also comes with filters to help developers and organizations make deployment decisions based on what‚Äôs important for them. Need a model with strong privacy guarantees for healthcare applications? Filter by privacy scores. Building a customer-facing application requiring consistent outputs? Prioritize reliability ratings.\nEvaluating both open and closed models with the exact same standards highlighted some interesting results: many open models actually outperform their closed counterparts in specific risk dimensions (particularly transparency, where open development practices shine).\nLet‚Äôs look at general trends:\nRisk distribution is polarized ‚Äì most models are strong, but mid-tier scores show elevated exposure\nhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/riskrubric/RiskRubric.png\nThe total risk scores range from 47 to 94, with a median of 81 (on a 100 points). Most models cluster in the ‚Äúsafer‚Äù range (54% are A or B level), but a long tail of underperformers drags the average down. That split shows a polarization: models tend to be either well-protected or in the middle-score range, with fewer in between.\nThe models concentrated in the 50‚Äì67 band (C/D range) are not outright broken, but they do provide only medium to low overall protection. This band represents the most practical area of concern, where security gaps are material enough to warrant prioritization.\nWhat this means: Don‚Äôt assume the ‚Äúaverage‚Äù model is safe. The tail of weak performers is real ‚Äì and that‚Äôs where attackers will focus. Teams can use composite scores to set a minimum threshold (e.g. 75) for procurement or deployment, ensuring outliers don‚Äôt slip into production.\nSafety risk is the ‚Äúswing factor‚Äù ‚Äì but it tracks closely with security posture\nhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/riskrubric/Safety.png\nThe Safety & Societal pillar (e.g. harmful output prevention) shows the widest variation across models. Importantly, models that invest in security hardening (prompt injection defenses, policy enforcement) almost always score better on safety as well.\nWhat this means : Strengthening core security controls goes beyond preventing jailbreaks, but also directly reduces downstream harms! Safety seems like it is a byproduct of robust security posture.\nGuardrails can erode transparency ‚Äì unless you design for it\nStricter protections often make models less transparent to end users (e.g. refusals without explanations, hidden boundaries). This can create a trust gap: users may perceive the system as ‚Äúopaque‚Äù even while it‚Äôs secure.\nWhat this means : Security shouldn‚Äôt come at the cost of trust. To balance both, pair strong safeguards with explanatory refusals, provenance signals, and auditability . This preserves transparency without loosening defenses.\nAn updating results sheet can be accessed here\nWhen risk assessments are public and standardized, the entire community can work together to improve model safety. Developers can see exactly where their models need strengthening, and the community can contribute fixes, patches, and safer fine-tuned variants. This creates a virtuous cycle of transparent improvement that's impossible with closed systems. It also helps the community at large understand what works and does not, safety wise, by studying best models.\nIf you want to take part in this initiative, you can submit your model for evaluation (or suggest existing models!) to understand their risk profile!\nWe also welcome all feedback on the assessment methodology and scoring framework"}
{"url": "https://huggingface.co/blog/inference-providers-publicai", "title": "Public AI on Hugging Face Inference Providers üî•", "content": "https://cdn-avatars.huggingface.co/v1/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/6192895f3b8aa351a46fadfd/2VifD-AAKYk24AUmfSr_X.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/1655385361868-61b85ce86eb1f2c5e6233736.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/1649143001781-624bebf604abc7ebb01789af.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/6270d2ddbcef985363d774fa/HOKAxx_FKVRF-87WpGQbF.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/638f308fc4444c6ca870b60a/Q11NK-8-JbiilJ-vk2LAR.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wbfw2veGoe1vMONqJbfzR.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/6822b890692919b8cc12fb58/vtOfs_qSBFb2bdPzAmyUE.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/68928e91792db00ed9fd30ea/wGfU5hSnrkoWJNnXnR7u2.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/6822b890692919b8cc12fb58/vtOfs_qSBFb2bdPzAmyUE.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/6192895f3b8aa351a46fadfd/2VifD-AAKYk24AUmfSr_X.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/5dd96eb166059660ed1ee413/NQtzmrDdbG0H8qkZvRyGk.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/1608146735109-5fcfb7c407408029ba3577e2.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/1659336880158-6273f303f6d63a28483fde12.png\nhttps://huggingface.co/blog/assets/inference-providers/welcome-publicai.jpg\nWe're thrilled to share that Public AI is now a supported Inference Provider on the Hugging Face Hub!\nPublic AI joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub‚Äôs model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers.\nThis launch makes it easier than ever to access public and sovereign models from institutions like the Swiss AI Initiative and AI Singapore ‚Äî right from Hugging Face. You can browse Public AI‚Äôs org on the Hub at https://huggingface.co/publicai and try trending supported models at https://huggingface.co/models?inference_provider=publicai&sort=trending .\nThe Public AI Inference Utility is a nonprofit, open-source project. The team builds products and organizes advocacy to support the work of public AI model builders like the Swiss AI Initiative and AI Singapore, among others.\nThe Public AI Inference Utility runs on a distributed infrastructure that combines a vLLM-powered backend with a deployment layer designed for resilience across multiple partners. Behind the scenes, inference is handled by servers exposing OpenAI-compatible APIs on vLLM, deployed across clusters donated by national and industry partners. A global load-balancing layer ensures requests are routed efficiently and transparently, regardless of which country‚Äôs compute is serving the query.\nFree public access is supported by donated GPU time and advertising subsidies, while long-term stability is intended to be anchored by state and institutional contributions. You can learn more about Public AI‚Äôs platform and infrastructure at https://platform.publicai.co/ .\nYou can now use the Public AI Inference Utility as an Inference Provider on Hugging Face. We're excited to see what you'll build with this new provider.\nRead more about how to use Public AI as an Inference Provider in its dedicated documentation page .\nSee the list of supported models here .\nhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-providers/user-settings-updated.png\nhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-providers/explainer.png\nhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-providers/model-widget-updated.png\nThe following example shows how to use Swiss AI's Apertus-70B using Public AI as the inference provider. You can use a Hugging Face token for automatic routing through Hugging Face, or your own Public AI API key if you have one.\nNote: this requires using a recent version of huggingface_hub (>= 0.34.6).\nAt the time of writing, usage of the Public AI Inference Utility through Hugging Face Inference Providers is free of charge. Pricing and availability may change.\nHere is how billing works for other providers on the platform:\nFor direct requests, i.e. when you use the key from an inference provider, you are billed by the corresponding provider. For instance, if you use a Public AI API key you're billed on your Public AI account.\nFor routed requests, i.e. when you authenticate via the Hugging Face Hub, you'll only pay the standard provider API rates. There's no additional markup from us; we just pass through the provider costs directly. (In the future, we may establish revenue-sharing agreements with our provider partners.)\nImportant Note ‚ÄºÔ∏è PRO users get $2 worth of Inference credits every month. You can use them across providers. üî•\nSubscribe to the Hugging Face PRO plan to get access to Inference credits, ZeroGPU, Spaces Dev Mode, 20x higher limits, and more.\nWe also provide free inference with a small quota for our signed-in free users, but please upgrade to PRO if you can!\nWe would love to get your feedback! Share your thoughts and/or comments here: https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/49"}
{"url": "https://huggingface.co/blog/lerobot-datasets-v3", "title": "LeRobotDataset:v3.0: Bringing large-scale datasets to lerobot", "content": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857146757-5e67bdd61009063689407479.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/1594214747713-5e9ecfc04957053f60648a3e.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/1594144055859-5ee3a7cd2a3eae3cbdad1305.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/1617264212503-603d25b75f9d390ab190b777.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/60a551a34ecc5d054c8ad93e/dhcBFtwNLcKqqASxniyVw.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/1648655841478-noauth.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/63d67eac6f49aa8230601996/djvtWdy718whUgh7tu1Ko.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/668bd06dd58b51a628566d80/II7Yr5dT5ItMrpoMkQEy3.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/1594214747713-5e9ecfc04957053f60648a3e.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/67d7dea1786ddcb3af5a44b3/gEgXTH4oO91GIzjHR-yrb.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/65f9d37113336392bad1e49c/B0Fxwconnu7lvtjBz4Ruq.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/65eab3c432efd8afebc5aee9/7kIg3Dpd2Sdwv-tgFOvtn.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/62f857fbb9fda55613ce80d9/d7bRniKLmOt-iFN07k1Su.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/65fcb7f133a3d6f126772121/BvVbNqnlQgDr2f_9dm5Es.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/64c255b2254239173af0570a/xQtKvcQynqrIc52QgvICp.jpeg\nhttps://huggingface.co/avatars/a464d228328719274a20121e2a82f703.svg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/noauth/CXvSv2l15uPkMQL_HBRDF.png\nLeRobotDataset is a standardized dataset format designed to address the specific needs of robot learning, and it provides unified and convenient access to robotics data across modalities, including sensorimotor readings, multiple camera feeds and teleoperation status.\nOur dataset format also stores general information regarding the way the data is being collected ( metadata ), including a textual description of the task being performed, the kind of robot used and measurement details like the frames per second at which both image and robot state streams are sampled.\nMetadata are useful to index and search across robotics datasets on the Hugging Face Hub!\nWithin lerobot , the robotics library we are developing at Hugging Face, LeRobotDataset provides a unified interface for working with multi-modal, time-series data, and it seamlessly integrates both with the Hugging Face and Pytorch ecosystems.\nThe dataset format is designed to be easily extensible and customizable, and already supports openly available datasets from a wide range of embodiments‚Äîincluding manipulator platforms such as the SO-100 arms and ALOHA-2 setup, real-world humanoid data, simulation datasets, and even self-driving car data!\nYou can explore the current datasets contributed by the community using the dataset visualizer ! üîó\nBesides scale, this new release of LeRobotDataset also enables support for a streaming functionality, allowing to process batches of data from large datasets on the fly, without having to download prohibitively large collections of data onto disk.\nYou can access and use any dataset in v3.0 in streaming mode by using the dedicated StreamingLeRobotDataset interface!\nStreaming datasets is a key milestone towards more accessible robot learning, and we are excited about sharing it with the community ü§ó\nhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lerobotdataset-v3/asset1datasetv3.png\nhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lerobotdataset-v3/streaming-lerobot.png\nlerobot is the end-to-end robotics library developed at Hugging Face, supporting real-world robotics as well as state of the art robot learning algorithms.\nThe library allows to record datasets locally directly on real-world robots, and to store datasets on the Hugging Face Hub.\nYou can read more about the robots we currently support here !\nLeRobotDataset:v3 is going to be a part of the lerobot library starting from lerobot-v0.4.0 , and we are very excited about sharing it early with the community. You can install the latest lerobot-v0.3.x supporting this new dataset format directly from PyPI using:\nFollow the community's progress towards a stable release of the library here ü§ó\nOnce you have installed a version of lerobot which supports the new dataset format, you can record a dataset with our signature robot arm, the SO-101, by using teleoperation alongside the following instructions:\nHead to the official documentation to see how to record a dataset for your use case.\nA core design choice behind LeRobotDataset is separating the underlying data storage from the user-facing API.\nThis allows for efficient serialization and storage while presenting the data in an intuitive, ready-to-use format. Datasets are organized into three main components:\nhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lerobotdataset-v3/dataset-v3-pillars.png\nTo support datasets with potentially millions of episodes (resulting in hundreds of millions/billions of individual frames), we merge data from different episodes into the same high-level structure.\nConcretely, this means that any given tabular collection and video will not contain information about one episode only, but a concatenation of the information available in multiple episodes.\nThis keeps the pressure on the file system manageable, both locally and on remote storage providers like Hugging Face.\nWe can then leverage metadata to gather episode-specific information, e.g. the timestamp a given episode starts or ends in a certain video.\nDatasets are organized as repositories containing:\nLeRobotDataset:v3.0 will be released with lerobot-v0.4.0 , together with the possibility to easily convert any dataset currently hosted on the Hugging Face Hub to the new v3.0 using:\nWe are very excited about sharing this new format early with the community! While we develop lerobot-v0.4.0 , you can still convert your dataset to the newly updated version by using the latest lerobot-v0.3.x supporting this new dataset format directly from PyPI using:\nNote that this is a pre-release, and generally unstable version. You can follow the status of the development of our next stable release here !\nThe conversion script convert_dataset_v21_to_v30.py aggregates the multiple episodes episode-0000.mp4, episode-0001.mp4, episode-0002.mp4, ... / episode-0000.parquet, episode-0001.parquet, episode-0002.parquet, episode-0003.parquet, ... into single files file-0000.mp4 / file-0000.parquet , and updates the metadata accordingly, to be able to retrieve episode-specific information from higher-level files.\nEvery dataset on the Hugging Face Hub containing the three main pillars presented above (Tabular and Visual Data, as well as relational Metadata), and can be accessed with a single line.\nMost robot learning algorithms, based on reinforcement learning (RL) or behavioral cloning (BC), tend to operate on a stack of observations and actions.\nFor instance, RL algorithms typically use a history of previous observations o_{t-H_o:t} , and\nBC algorithms are instead typically trained to regress chunks of multiple actions.\nTo accommodate for the specifics of robot learning training, LeRobotDataset provides a native windowing operation, whereby we can use the seconds before and after any given observation using a delta_timestamps argument.\nConveniently, by using LeRobotDataset with a PyTorch DataLoader one can automatically collate the individual sample dictionaries from the dataset into a single dictionary of batched tensors.\nYou can also use any dataset in v3.0 format in streaming mode, without the need to download it locally, by using the StreamingLeRobotDataset class.\nLeRobotDataset v3.0 is a stepping stone towards scaling up robotics datasets supported in LeRobot. By providing a format to store and access large collections of robot data we are making progress towards democratizing robotics, allowing the community to train on possibly millions of episodes without even downloading the data itself!\nYou can try the new dataset format by installing the latest lerobot-v0.3.x , and share any feedback on GitHub or on our Discord server ! ü§ó\nWe thank the fantastic yaak.ai team for their precious support and feedback while developing LeRobotDataset:v3. \nGo ahead and follow their organization on the Hugging Face Hub!\nWe are always looking to collaborate with the community and share early features. Reach out if you would like to collaborate üòä"}
{"url": "https://huggingface.co/blog/watermarking-with-gradio", "title": "Visible Watermarking with Gradio", "content": "https://cdn-avatars.huggingface.co/v1/production/uploads/1594144055859-5ee3a7cd2a3eae3cbdad1305.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/6051e59531c5be7f3dd5ebc9/T0o_eKK5gmZoG-qbbj3lM.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/1621947938344-noauth.png\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/60c8351eb935f5a38b2eaf65/S0oeerwpyYDehGQOodlOx.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/1624431552569-noauth.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/61fc476120c583345f868e83/wNKd3iNG8BbDcXGIf1k-T.jpeg\nhttps://cdn-avatars.huggingface.co/v1/production/uploads/1626214544196-60c757ea5f9a76ab3f844f12.png\nhttps://cdn-uploads.huggingface.co/production/uploads/60c757ea5f9a76ab3f844f12/DM6_nlPtgsfRytok2F1Ib.webp\nLast year, we shared a blogpost on watermarking , explaining what it means to watermark generative AI content, and why it's important. The need for watermarking has become even more critical as people all over the world have begun to generate and share AI-generated images, video, audio, and text. Images and video have become so realistic that they‚Äôre nearly impossible to distinguish from what you‚Äôd see captured by a real camera.  Addressing this issue is multi-faceted, but there is one, clear, low-hanging fruit üçá:\nTo help out, we at Hugging Face have made visible watermarking trivially easy: Whenever you create a Space like an app or a demo , you can use our in-house app-building library Gradio to display watermarks with a single command.\nFor images and video, simply add the watermark parameter, like so:\nWatermarks can be specified as filenames, and for images we additionally support open images or even numpy arrays, to work best with how you want to set up your interface. One option I particularly like is QR watermarks, which can be used to get much more information about the content, and can even be matched to the style of your image or video .\nYou can also add custom visible watermarks for AI-generated text, so that whenever it is copied, the watermark will appear.  Like so:\nThis automatically adds attribution when users copy text from AI responses, further aiding in AI transparency and disclosure for text generation.\nTry it all out today, build your own watermark, have fun!\nHappy Coding!\nAcknowledgements: Abubakar Abid and Yuvraj Sharma collaborated on this work and blog post."}
